<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://luo-jiaming.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://luo-jiaming.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-23T18:12:09+00:00</updated><id>https://luo-jiaming.github.io/feed.xml</id><title type="html">blank</title><subtitle>The academic profile of Bin Wu. </subtitle><entry><title type="html">Continual Learning of LLMs 调研</title><link href="https://luo-jiaming.github.io/blog/2025/continual-learning-of-llms-survey/" rel="alternate" type="text/html" title="Continual Learning of LLMs 调研"/><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>https://luo-jiaming.github.io/blog/2025/continual-learning-of-llms-survey</id><content type="html" xml:base="https://luo-jiaming.github.io/blog/2025/continual-learning-of-llms-survey/"><![CDATA[<p><br/></p> <h2 id="1-引言">1 引言</h2> <p>语言模型的持续学习研究从2017年左右开始一直与视觉模型一样并行地在展开，尽管前者热度相对后者较低一些，近年来LLM的持续学习还在起步阶段。LLM的持续学习主要分为上游的<strong>持续预训练</strong>和下游的<strong>持续微调</strong>。将预训练好的LLM应用至特定专业领域（medical、finance等）但不遗忘通用知识的<strong>持续领域适应预训练</strong>也有一些工作。</p> <p>持续预训练的最大现实意义在于可以让LLM随着语料库的更新与时俱进地掌握最新事实。尽管RAG已经作为不错的替代方案出现了，但显然RAG并不能一劳永逸地解决问题。但是由于并非所有团体都有能力做好持续预训练，目前的工作大多不太成熟。</p> <p>因为语言模型的下游应用更广，这也导致了LLM的持续微调出现了多种五花八门的setting，可以按照微调的方式和微调的任务进行划分。LLM所特有的setting包含持续指令微调、持续模型精炼（主要是持续模型编辑）和持续模型对齐（主要是持续人类反馈强化学习）。LLM的持续微调的热度较预训练更高些，也出现了一些相对成熟的工作。</p> <p>一个普遍的认知是优秀的特征表示能增强对遗忘的抗性，大多数工作都发现LLM对遗忘的抵抗性很强，只需简单的重放就能取得良好的效果。LLM还有能力自己生成伪数据而避免保存历史样本引发的存储和隐私问题。目前为止大多数工作的backbone在7B规模及以下，GPT2、lamma2、T5等。</p> <p>主要参考资料（可以参考的论文列表）：</p> <p>Continual Learning of Large Language Models: A Comprehensive Survey [<a href="https://arxiv.org/abs/2404.16789">paper</a>] [<a href="https://github.com/Wang-ML-Lab/llm-continual-learning-survey">code</a>]</p> <p><br/></p> <h3 id="11-motivation">1.1 Motivation</h3> <p>为什么LLM需要做持续学习？</p> <ol> <li> <p>LLM通过预训练获得的静态知识会过时，需要与时俱进地更新。</p> </li> <li> <p>LLM尽管通过预训练获得了通用知识，在特定领域仍需要微调进一步提升性能。（这种情况会多次遇到所以需要多次微调）</p> </li> </ol> <p>在这两种场景下都希望LLM能够掌握新的能力而不遗忘已经掌握的知识，并且只在新数据上训练来提高<strong>效率</strong>（相较于comian and retrain的做法明显节省开销，很多时候retrain模型或为某些任务单独微调存储模型是持续学习的低效替代方案）。更理想的情况会考虑利用持续学习带来的数据集划分减轻一些数据冲突（例如不同年份某些事实会变化）或通过顺序学习建模鼓励<strong>正向的知识迁移</strong>（例如先会学某些知识后一些其他知识学得更好）。</p> <p><strong>PS：</strong>我曾经也尝试概括过预训练模型需要持续学习的原因如下图所示：</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250210153912111.png" alt="image-20250210153912111" style="zoom:33%;"/></p> <p><br/></p> <h3 id="12-llm的持续学习场景分类">1.2 LLM的持续学习场景分类</h3> <p>鉴于LLM学习模式的特殊性，LLM的持续学习对以下两种持续性[1]进行要求，对应下图的纵轴和横轴：</p> <p><strong>纵向持续性：</strong>将LLM从大的general domain逐步迁移至小的specific domian的过程（通用预训练-&gt;领域适应预训练-&gt;进一步下游微调），在此过程中不能遗忘上游通用domian的知识。例如将通用LLM调整成医疗领域专用模型但不遗忘通用能力。</p> <p><strong>横向持续性：</strong>对同一垂直阶段进行水平划分，将训练划分为横跨时间或分布领域的多个训练阶段，不能遗忘历史训练阶段的知识，亦即传统持续学习要求的持续性。这对应着同一垂直阶段的LLM的不断更新，例如预训练阶段用随时间更新的数据更新模型或让模型连续学习多种下游任务。</p> <p>这种建模类似于[2]中提出的生产者-消费者结构，横轴代表生产者端的持续学习，不断发行新版本的LLM，而纵轴代表消费者端的持续学习，在得到每个发行版本的LLM后，可以将其向用户需要的应用领域上不断迁移。（我估计综述[1]讲故事的方式很大程度受到[2]启发）</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250209180722844.png" alt="image-20250209180722844" style="zoom:50%;"/></p> <p>[1] <a href="http://arxiv.org/abs/2404.16789">Continual Learning of Large Language Models: A Comprehensive Survey. arXiv24</a></p> <p>[2] <a href="https://arxiv.org/abs/2305.08702">Recyclable tuning for continual pre-training. arXiv23</a></p> <p><br/> 在此基础上，可以将持续学习场景按照纵向和横向分别进行分类。</p> <p><strong>垂直领域分类：</strong>按将LLM从通用领域向下游专业领域不断适应的不同的学习阶段划分为：Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT)。</p> <p><strong>水平领域分类：</strong>按同一垂直领域中持续学习的不同的训练阶段组织方式进行划分。这个分类方式类似于传统持续学习中任务增量、领域增量和类增量的划分方式。在CPT中包含temporal, content-level, and language-level。而在CFT中包含<strong>Continual Instruction Tuning</strong> (CIT), <strong>Continual Model Refinement</strong> (CMR), <strong>Continual Model Alignment</strong> (CMA)。这对应了为了满足不同需要的几种主流的微调场景，指令微调、人类反馈强化学习和模型对齐。</p> <p><br/></p> <h3 id="13-llm的持续学习的遗忘">1.3 LLM的持续学习的遗忘</h3> <p><strong>纵向遗忘：</strong>在纵向学习的过程中，模型会在学习下游任务的同时遗忘上游知识。由于下游任务与上游任务的异构性（学习目标甚至模型结构不同）会加剧对上游知识的遗忘，最主要指的是对预训练通用知识的遗忘。同时由于上游数据通常无法再获取（上游数据可能由供给者私有），阻止了重放历史数据这一有效防遗忘手段的应用。但是如今优质公开数据集很多，我相信大部分的模型预训练数据本身重叠度很大，用合适的方式从公开数据集中采样可以构建<strong>代理</strong>预训练数据集。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250210160054632.png" alt="image-20250210160054632" style="zoom: 50%;"/></p> <p><strong>横向遗忘：</strong>这种遗忘就是传统持续学习中的遗忘。当任务序列较长或不同的训练阶段的数据包含不同程度的领域偏移（domian shifit）时会尤为严重。其中长任务序列场景我认为在现实中比较少见。</p> <p><br/></p> <h2 id="2-持续预训练-cpt">2 持续预训练 CPT</h2> <p>在持续预训练中，考虑的场景并非是一口气在一个静态的数据集上预训练完。而是按照数据的<strong>时期</strong>（Temporal）、<strong>内容</strong>（Content）或<strong>语种</strong>（Language）将预训练数据集划分并构建多个阶段的连续训练。对持续学习训练阶段的划分方式而言，为了更好地衡量遗忘，一般会保证每个阶段的训练数据不重叠，例如传统图像分类任务中每个训练阶段包含不相交的图像类别。其中按时期划分数据更贴近实际应用中不断用新数据更新LLM的场景。</p> <p>事实上，我看到的CPT工作种很少有提出新的预训练技术的，大多是基于简单基线的测评工作，简单基线可能是直接连续训练、基于简单的传统CL方法或使用PEFT风格的方法。backbone一般基于BERT系列或GPT-2家族或lamma-2。传统持续学习的方法往往是与全参数微调结合使用的，而全参数微调本身会导致比较严重的遗忘，效果不如使用LoRA或Adapter等结构。我认为LoRA或Adapter不适用于长任务序列。尤其是adpter如果需要为每个任务微调一个则在测试时需要知道任务id才能用。</p> <p><br/></p> <h3 id="21-temporal-incremental">2.1 Temporal Incremental</h3> <p>在静态语料库上预训练的模型需要与时俱进地更新知识解决<strong>temporal misalignment</strong>问题。与直接的<strong>combine and retrain</strong>不同，持续学习探索只在新数据上训练的场景来节省开销。与传统持续学习不同的是，模型需要保持时间无关信息的记忆（e.g., 奥巴马的生日），同时更新一些与时间有关的过时信息（e.g., 今年的美国总统是谁），并不是全盘保持记忆。</p> <p><strong>PS：</strong>我认为按时期划分预训练数据集也许可以减轻大时间跨度语料库中因时间变化引起的事实冲突导致的一些问题。</p> <p><br/> 关于此领域的一些问题：</p> <p>Q1：LLM知识的过时与幻觉有怎样的联系？</p> <p>Q2：现在工业界的LLM也有纠正错误或过时知识的手段：例如RAG可以使LLM在推理时获取最新信息。使用Prompt也可以告知LLM某些信息。或是使用LoRA+Model Soup来进行长期更新。这些也需要了解。</p> <p><br/> 下面罗列一些具体的工作：</p> <p><br/></p> <h4 id="towards-continual-knowledge-learning-of-language-modelsiclr-2022paper-link">Towards Continual Knowledge Learning of Language Models（ICLR 2022）[<a href="https://arxiv.org/abs/2110.03215">paper link</a>]</h4> <p>这是一个benchmaek性质的工作。LLM按时期划分持续预训练时，将LLM需要掌握的知识划分为三种：（1）InvariantLAMA，与时间无关的始终成立的知识，不该被遗忘；（2）UpdateLAMA，需要与时俱进被更新的知识；（3）NewLAMA，未在过去语料库中出现的新学习的知识。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250212152402774.png" alt="image-20250212152402774" style="zoom:40%;"/></p> <p>基于此，这篇文章构建了新的<strong>benchmark</strong> CKL（Continual Knowledge Learning），只包含预训练后的一次继续预训练。其中backbone的预训练数据集为\(D_0\)，而一个远小于\(D_0\)的最新爬取的预料库作为\(D_1\)。用填空风格的LAnguage Model Analysis (LAMA) 作为评估任务，分别构建了评估<strong>保持时间无关知识</strong>（\(D_0 \cap D_1\)）、<strong>更新过时知识</strong>（\(D_0\)与\(D_1\)冲突）、<strong>获取新知识</strong>（\(D_1 - D_0\)）的测试集。</p> <p>在实验结果中发现使用额外的结构（如LoRA或Adapters）能取得较使用传统的重放或正则化更好的效果。个人认为重放方法对数据的选择很关键，否则可能会重放过时的知识，不利于新知识的学习或更新。同时在训练过程中能重放的数据量远小于预训练本身的数据量，随机采样的数据不一定能代表整个预训练数据集。</p> <p><br/></p> <h4 id="temporalwiki-a-lifelong-benchmark-for-training-and-evaluating-ever-evolving-language-models-emnlp-2022-paper-link">TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models (EMNLP 2022) [<a href="https://arxiv.org/abs/2204.14211">paper link</a>]</h4> <p>这篇工作通过英文维基Wikidata<strong>每月自动更新的快照</strong>构建自动更新的持续学习benchmark，而不同于传统的实质上仍是静态的benchmark。每个新的训练集由更新后的快照所包含的新文章和旧文章的修改部分组成。为了衡量稳性性-可塑性平衡，测试集也由改变的和未改变的部分组成（根据事实元组构建）。</p> <p>主要实验结果如下，衡量了在未更新部分和更新部分上较初始模型的相对困惑（越低越好）。发现相较PEFT方法，各种全参数微调的方法新任务学习得更好但也遗忘更多。这里RecAdam[1]是应用了正则化技术的CL方法，而Mix-review[2]是应用了重放技术的CL方法。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250212212016567.png" alt="image-20250212212016567" style="zoom:40%;"/></p> <p>[1] Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting (EMNLP 2020) [<a href="https://arxiv.org/abs/2004.12651">paper link</a>]</p> <p>[2] Analyzing the Forgetting Problem in the Pretrain-Finetuning of Dialogue Response Models (EACL 2021) [<a href="https://arxiv.org/abs/1910.07117">paper link</a>]</p> <p><strong>PS：</strong>对于CKL和TemporalWiki这两个benchmark，评估的backbone模型还是停留在T5或GPT-2时代，评估的任务也比较单一和简单。如果能建立更加全面的评估，也许能分析出模型的遗忘具体发生在哪些方面，例如knowledge或reliability等。</p> <p><br/></p> <h4 id="time-aware-language-models-as-temporal-knowledge-bases-naacl-2022-paper-link">Time-Aware Language Models as Temporal Knowledge Bases (NAACL 2022) <a href="https://arxiv.org/abs/2106.15110">[paper link]</a></h4> <p>TempoT5提出在每条事实性训练文本前<strong>显式地加上时间标识</strong>（年份）在预训练时增强模型对时间的感知，从而能减轻时期变化引发的遗忘。这样做模型不再混淆大时间跨度下的预训练语料中因时间变化引起的事实冲突。同时，当只用更新的事实更新模型时，模型也不容易遗忘无需被更新的事实。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250213164055310.png" alt="image-20250213164055310" style="zoom:40%;"/></p> <p>这篇文章的预训练语料是CUSTOMNEWS因而可以从新闻的url中直接提取出时间。但是在很多其他语料中也许很难直接获取时间信息？</p> <p><br/></p> <h4 id="tic-clip-continual-training-of-clip-models-iclr-2024-paper-link">TiC-CLIP: Continual Training of CLIP Models (ICLR 2024) [<a href="https://arxiv.org/abs/2310.16226">paper link</a>]</h4> <p>Apple的一篇Solid Work，将爬取到的数据按时间划分<strong>预训练</strong>CLIP，这个setting的目标是持续更新CLIP的知识来应对CLIP的temporal misalignment问题。提出了一个基于重放的简单baseline。</p> <p>我认为LLM的CPT需要一个像这样的benchmark，包含按年份组织的数据且涵盖广泛的领域。</p> <p><br/></p> <h3 id="22-content-incremental">2.2 Content Incremental</h3> <p>每个持续学习任务的训练语料来自不同的<strong>专业domian</strong>（因此类似传统continual learning的domain incremental，但是传统CIL中需要推理测试数据来自哪个domain），如News, Social Media, Scientific Papers等。我觉得NLP领域对domain的定义似乎有点模糊？对于这种问题，引入MoE结构似乎比较符合直觉。</p> <p><br/></p> <h4 id="investigating-continual-pretraining-in-large-language-models-insights-and-implications-arxiv-2024paper-link">Investigating Continual Pretraining in Large Language Models: Insights and Implications （arXiv 2024）[<a href="https://arxiv.org/abs/2402.17400">paper link</a>]</h4> <p>这篇文章根据包含最多256 domian的M2D2数据集中的159个domain构建了很长的任务序列。测评的每个模型都是预训练好的。按照任务相似度组织（相似的任务相邻）或随机顺序组织任务序列。发现lamma2在CL后的困惑度比zero-shot时更高了:sweat:。没有和joint train比较拿不准效果好坏。个人感觉这篇工作还是缺乏由深度有价值的结论。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250215174610923.png" alt="image-20250215174610923" style="zoom:50%;"/></p> <p><br/></p> <h4 id="elle-efficient-lifelong-pre-training-for-emerging-data-acl-2022paper-link">ELLE: Efficient Lifelong Pre-training for Emerging Data （ACL 2022）[<a href="https://arxiv.org/abs/2203.06311">paper link</a>]</h4> <p>by THU Yujia Qin</p> <p>这篇文章比较贴近传统持续学习工作的风格，给整个任务的设定给出了明确的定义，并设计了基于参数扩展和重放的基线。每个task的语料来自不同的文本领域（新闻、网页、文学作品等）。文中使用5个数据集构建这样的任务序列。</p> <p>为了让模型掌握更多的知识。在学习每个新任务前需要先扩展模型，包括宽度和深度的扩展。宽度扩展要求输出不改变（但被复制），从矩阵运算的角度可以通过线性变换与复制重排参数实现。深度扩展则是随机选择一些层将其复制直接置于原来的位置之后。这样可以在几乎不改变原始输出的前提下扩展模型。为了进一步保证扩展的可靠性，扩展后会通过<strong>重放</strong>对齐输出。同时为了让模型更好地使用不同的domain，会为每个domain专门配置一组可学习的提示。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250214172608730.png" alt="image-20250214172608730" style="zoom:50%;"/></p> <p><br/></p> <h4 id="recyclable-tuning-for-continual-pre-training-findings-of-acl-2023-paper-link">Recyclable Tuning for Continual Pre-training (Findings of ACL 2023) [<a href="https://arxiv.org/abs/2305.08702">paper link</a>]</h4> <p>by THU Yujia Qin</p> <p>一篇有趣的工作。LLM本身的预训练权重会被发行方不断的更新。而当更强的新版预训练模型被推出时，用户基于旧版模型在下游任务上微调出的权重可能会被浪费。而直接将这些旧版模型在下游任务微调的参数增量加到新版模型上也能带来明显的下游任务提升。这是一个<a href="https://arxiv.org/abs/1912.05671">Linear Mode Connectivity</a>的问题，作者认为持续预训练的LLM的之间具有<strong>线性连接性</strong>和<strong>函数相似性</strong>（文中指attention head对相同输入具有相似输出）。针对这个特性，作者提出使用新预训练模型+旧微调模型增量作为后续在<strong>同一任务</strong>上微调的初始化。文章只讨论了同一下游任务的情形，我觉得还可以扩展到相似domain的任务。</p> <p><br/></p> <h4 id="demix-layers-disentangling-domains-for-modular-language-modeling-naacl-2022-paper-link">DEMix Layers: Disentangling Domains for Modular Language Modeling (NAACL 2022) <a href="https://arxiv.org/abs/2108.05036">[paper link]</a></h4> <p>by Facebook AI Research</p> <p>提出使用若干门的feedforward networks FFN代替transformer block中的FFN。在训练时每个domain都会训练专门的FFN。在推理时与其他MoE方法不同的是，设计了一种<strong>无需参数</strong>的加权方法（按domain全概率展开+Bayes）。这种加权估计在传统持续学习中比较常见，难点在于如何估计每个领域的先验概率，文章介绍了三种做法（uniform、ema、cache）。</p> <p><br/></p> <h4 id="lifelong-language-pretraining-with-distribution-specialized-experts-icml-2023-paper-link">Lifelong Language Pretraining with Distribution-Specialized Experts (ICML 2023) [<a href="https://arxiv.org/pdf/2305.12281">paper link</a>]</h4> <p>引入稀疏门控的MoE机制（在训练和推理时都激活最好的两个专家）。与DEMix Layers不同的是，因为专家的参数被学习新任务时被改写而引入了遗忘，提出使用知识蒸馏来保持历史知识。遗憾的是尽管没有采用等差增长的expert，这篇文章的expert扩展方式仅仅是选择了最优实验结果而未作过多讨论。</p> <p><br/></p> <h4 id="lifelong-pretraining-continually-adapting-language-models-to-emerging-corpora-naacl-2022paper-link">Lifelong pretraining: Continually adapting language models to emerging corpora （NAACL 2022）[<a href="https://arxiv.org/abs/2110.08534">paper link</a>]</h4> <p>还没看。</p> <p><br/></p> <h3 id="23-language-incremental">2.3 Language Incremental</h3> <p>模型在多个包含完全不同语种的预训练数据集上连续做预训练。也许和multi-lingual的工作有所关联。目前只看到几篇不太成熟的工作。</p> <h4 id="continual-learning-under-language-shift-tsd-2024paper-link">Continual Learning Under Language Shift （TSD 2024）[<a href="https://arxiv.org/abs/2311.01200">paper link</a>]</h4> <p>用英语、丹麦，冰岛和挪威语四种语种构建任务序列并从头持续预训练一个GPT架构的模型。观察到学习完英语后，再学习另外三种语言的效果变得比直接学习每种语言更好了。但是遗忘仍然存在。</p> <p><br/></p> <h4 id="examining-forgetting-in-continual-pre-training-of-aligned-large-language-models-arxiv-2024-ongoingpaper-link">Examining forgetting in continual pre-training of aligned large language models （arXiv 2024 ongoing）[<a href="https://arxiv.org/abs/2401.03129">paper link</a>]</h4> <p>Hung-Yi Lee手下学生的工作。在将Llama-2-7b-chat在10亿token的繁体中文预料上持续预训练后发现出现严重的重复（repetition）问题。由于Llama-2-7b-chat本身就是由Llama-2-7b微调减轻重复性问题得到（align），所以作者认为这是一种灾难性遗忘现象。而使用\((IA)^3\)代替直接改变预训练模型的参数能减轻这一问题。</p> <p><br/></p> <h4 id="overcoming-catastrophic-forgetting-in-massively-multilingual-continual-learning-findings-of-acl-2023paper-link">Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning （Findings of ACL 2023）[<a href="https://arxiv.org/abs/2305.16252">paper link</a>]</h4> <p>现实的多语种系统应该要具备不断学习新语言的能力。在51个语言的三种任务上测评。发现采用学习率的re-warmup就能大幅减轻遗忘。</p> <p><br/></p> <h3 id="24-others">2.4 Others</h3> <p>任务组织没有明显特点的其他CPT工作。</p> <p><br/></p> <h4 id="continual-pre-training-of-large-language-models-how-to-rewarm-your-model-icmlw-2023-paper-link">Continual Pre-Training of Large Language Models: How to (re)warm your model? (ICMLW 2023) [<a href="https://arxiv.org/abs/2308.04014">paper link</a>]</h4> <p>将在Pile数据集上预训练的Pythia 410M模型在SlimPajama模型上继续预训练。模拟的场景是将旧模型在新出的更优质的数据集上继续预训练来增强它。这篇文章主要讨论如何在持续学习的过程中找到重新warm-up的学习率方案，属于<strong>偏工程的调参心得</strong>。</p> <p><br/></p> <h4 id="rho-1-not-all-tokens-are-what-you-need-neurips-2024-oral-paper-link">Rho-1: Not All Tokens Are What You Need (NeurIPS 2024 Oral) [<a href="https://arxiv.org/abs/2404.07965">paper link</a>]</h4> <p>在continual pretraining的过程中，由于模型已经具有某些知识，所以每个token的loss变化是不同的，共有四种变化模式（High-&gt;High,HIgh-&gt;low,low-&gt;high,low-&gt;low）。这篇工作提出先训练一个参考模型至收敛，然后将后续的模型<strong>只在loss明显高于参考模型的token上选择性训练</strong>，最后得到的模型具有更强的能力并花费更少的开销。不过这依旧建立在需要一个强参考模型的前提下。</p> <p><br/></p> <h4 id="cem-a-data-efficient-method-for-large-language-models-to-continue-evolving-from-mistakes-arxiv-2024paper-link">CEM: A Data-Efficient Method for Large Language Models to Continue Evolving From Mistakes （arXiv 2024）[<a href="https://arxiv.org/pdf/2404.08707">paper link</a>]</h4> <h4 id="take-the-bull-by-the-horns-hard-sample-reweighted-continual-training-improves-llm-generalization-arxiv-2024-paper-link">Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization (arXiv 2024) [<a href="https://arxiv.org/pdf/2402.14270">paper link</a>]</h4> <p><br/></p> <h2 id="3-持续领域适应预训练">3 持续领域适应预训练</h2> <p>侧重于强调在不遗忘同用知识的前提下将模型在某一特定的专业领域进一步预训练（一般只有一个阶段），但仍不能遗忘通用知识。实际上只是持续预训练的分支，但是根据适应的domain不同（medical、finance、code、science等），仍有许多工作能细分出许多类别，故单独列为一项。</p> <p>（工作太多，看不过来了，后续有空再看）</p> <p><br/></p> <h2 id="4-持续微调">4 持续微调</h2> <p>持续微调是LLM应用垂直领域的最后一层。</p> <p>奇妙的是很多工作都claim自己达到了自己setting的upper bound。</p> <p><br/></p> <h3 id="41-general-continual-fine-tuning-general-cft">4.1 General Continual Fine-Tuning (General CFT)</h3> <p>使用的是全参数微调或PEFT或probing</p> <p><br/></p> <h4 id="achieving-forgetting-prevention-and-knowledge-transfer-in-continual-learning-neurips-2021paper-link">Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning （NeurIPS 2021）[<a href="https://arxiv.org/abs/2112.02706">paper link</a>]</h4> <p>将预训练的Bert持续微调应用于下游<strong>分类</strong>任务，基于DIL和CIL的方式构建任务序列。采的方法是类MoE机制组合的类Adapter结构。</p> <p><br/></p> <h4 id="can-bert-refrain-from-forgetting-on-sequential-tasks-a-probing-study-iclr-2023-paper-link">Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study （ICLR 2023） [<a href="https://arxiv.org/abs/2303.01081">paper link</a>]</h4> <p>发现Bert在下游文本分类任务上微调时任务内的分类不会被扰乱，但是任务间的决策边界重叠了。所以在被告知任务id后进行分类或linear probing性能没有太大退化。通过简单的稀疏重放，BERT在TIL设置上表现出强大的抗遗忘能力。</p> <p><br/></p> <h4 id="lfpt5-a-unified-framework-for-lifelong-few-shot-language-learning-based-on-prompt-tuning-of-t5-iclr-2022-paper-link">LFPT5, A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5 (ICLR 2022) [<a href="https://arxiv.org/abs/2110.07298">paper link</a>]</h4> <p>这篇文章研究的是更复杂的持续学习场景，包括学习不同的下游任务（优化目标不同）与同一类型任务中的不同domain。将所有任务都重建为text2text的形式。用不同的prompt来学习不同类型的任务，同一任务的不同种类则用同一prompt持续学习。（这样做不同任务之间似乎不会引起彼此的遗忘）。但是由于需要不同的输出头，不同任务的学习其实是完全隔离的（指令微调可以统一学习）。为了避免学习同一任务的不同domain引起的遗忘，让LLM通过重建loss<strong>自己学习生成历史样本</strong>进行生成式重放。关于few-shot没有做特别设计，仅仅利用了预训练语言模型的few-shot学习能力。</p> <p>一边学习下游任务一边学习重建历史样本也是LM独有的能力，在<a href="https://arxiv.org/abs/1909.03329">Lamol</a>（ICLR 2019）种被首次提出。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250217152802469.png" alt="image-20250217152802469" style="zoom: 50%;"/></p> <p><br/></p> <h4 id="mofo-momentum-filtered-optimizer-for-mitigating-forgetting-in-llm-fine-tuning-arxiv-2024-paper-link">MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning (arXiv 2024) [<a href="https://arxiv.org/abs/2407.20999">paper link</a>]</h4> <p>ICLR25拒稿。为了解决模型在下游任务微调时遗忘预训练通用知识，提出只更新动量最大的\(\alpha\%\)参数（类似于选择性微调），从而保持模型落在距离预训练权重较近的局部最优内。这也是基于持续学习的一个经典认知做的。但是我认为减少无用参数更新来减轻遗忘只是一种平凡的现象。当任务间domain gap很大时，这种做法可能就会收效甚微。</p> <p>这方面有两个比较偏理论的问题：1）特定规则下的选择性调参一定能收敛得和全参数微调相当吗？2）为什么某一特定规则的选择性调参在收敛后较全参数微调距离预训练权重更接近？但是文章没给出很好的回答。</p> <p><br/></p> <h4 id="preserving-generalization-of-language-models-in-few-shot-continual-relation-extraction-emnlp-2024paper-link">Preserving Generalization of Language models in Few-shot Continual Relation Extraction （EMNLP 2024）[<a href="https://arxiv.org/abs/2410.00334">paper link</a>]</h4> <p>Few-shot Continual Relation Extraction作为一种专门的赛道似乎现在有比较明确的benchmark和baseline。在few-shot learning中可能因为数据较少引起过拟合，而过拟合本身会加重遗忘。而已有做法往往是用Bert 等backbone + 下游任务上训练的专门分类器完成这个任务，这样的分类器很容易对新任务产生偏见。这篇工作提出最大化backbone预训练的输出头和下游任务分类器的互信息（这里我没太看懂，应该是两个分布的互信息？）来缓解过拟合。尽管标题上提到了保持泛化，但<strong>没有评估模型的通用知识</strong>。</p> <p><br/></p> <h4 id="parameterizing-context-unleashing-the-power-of-parameter-efficient-fine-tuning-and-in-context-tuning-for-continual-table-semantic-parsing-neurips-2023paper-link">Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing （NeurIPS 2023）[<a href="https://arxiv.org/abs/2310.04801">paper link</a>]</h4> <p>针对特定下游任务Table Semantic Parsing的工作，采用上下文学习和PEFT结合的方式。</p> <p><br/></p> <h4 id="learn-or-recall-revisiting-incremental-learning-with-pre-trained-language-models-acl-2024-oralpaper-link">Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models （ACL 2024 Oral）[<a href="https://arxiv.org/abs/2312.07887">paper link</a>]</h4> <p>研究下游分类任务的持续微调。这篇文章通过linear probing分析发现即使是全量持续微调预训练语言模型，在持续学习过程中尽管下游任务表现观测到了遗忘，其在下游任务上的linear probing表现没有明显退化。说明预训练模型backbone的特征表示具有较强抗遗忘能力，而分类器发生严重遗忘，即传统CL中的<strong>class imbalance</strong>问题。作者通过实验（将持续微调中的分类器与直接linear probing得到的分类器对比）分析出分类器遗忘的原因是旧类对应的分类器权重被大幅更改了。最后根据观察结论提出一些基于分类器的改进方案，例如冻结旧分类器权重，采用余弦线性分类器消除模长影响等。总的来说算是一篇不错的empirical study。</p> <p><br/></p> <h3 id="42-continual-instruction-tuning-cit">4.2 Continual Instruction Tuning (CIT)</h3> <p>指令微调是语言模型的一种特殊微调方式，训练数据为表述任务要求的指令（指令中可能含有少量示例）和输入输出的样本。指令微调可行是因为许多NLP任务都可以被统一至text2text的形式。持续指令微调可以允许模型在同一形式的训练下连续学习异构的下游任务（分类、生成等）且减轻对标注数据的依赖，这在其他领域的CL中就很难实现。有些工作也使用仅包含更少上下文示例的纯指令微调来进一步减轻标注负担。由于CIT的backbone很多是zero shot learner，评估时除了微调的下游任务表现，还需要评估分布外的泛化能力。最新工作的benchmark一般基于Sup-NatInst数据集构建。</p> <p>Q: 指令微调还允许了异构任务的joint training，这种形式的multi-task learning和分开训练比较效果如何？</p> <p><br/></p> <h4 id="contintin-continual-learning-from-task-instructions-acl-2022-paper-link">ConTinTin: Continual Learning from Task Instructions （ACL 2022） [<a href="https://arxiv.org/abs/2203.08512">paper link</a>]</h4> <p>首个研究持续指令微调的工作，给出持续指令微调应该要满足的要求。基于<strong>NaturalInstructions</strong>数据集构建了benchmark，将其中的61个任务划分为初始化任务和持续指令微调的任务。其中初始化任务的预训练数据包含有标注的样本而持续指令微调的任务只包含指令而<strong>不包含样本（但指令中可能含有正负示例）</strong>，想要探讨的问题就是通过指令微调降低对标注数据的依赖。采用负样本训练和重放结合的方式，超过了比较的基线LAMOL。</p> <p>后续的工作似乎不再考虑只从指令学习的问题，而都会采用一定量的样本。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250218162259941.png" alt="image-20250218162259941" style="zoom:50%;"/></p> <p><br/></p> <h4 id="fine-tuned-language-models-are-continual-learners-emnlp-2022paper-link">Fine-tuned Language Models are Continual Learners （EMNLP 2022）[<a href="http://arxiv.org/abs/2205.12393">paper link</a>]</h4> <p>T5在50个数据集上通过大规模指令微调得到具有强大零样本泛化能力的T0，这篇工作探索继续在8个数据集上持续指令微调T0得到<strong>CT0</strong>，基于1%重放率的稀疏重放（1k个重放样本）。8个数据集对应Text Simplification、Headline Generation with Constraint、Haiku Generation等8种不同任务，但是通过指令微调可以以统一的形式学习。在实验种作者发现重放对保持泛化能力尤其重要，这是因为重放数据源包括了T5-&gt;T0的微调数据。最终观察到模型取得了匹敌upper bound的良好下游任务表现和泛化能力。</p> <p>和ConTinTin比较相似但是同期工作。文章使用的指令的例子：</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250218151346953.png" alt="image-20250218151346953" style="zoom:50%;"/></p> <p><br/></p> <h4 id="large-scale-lifelong-learning-of-in-context-instructions-and-how-to-tackle-it-acl-2023paper-link">Large-scale Lifelong Learning of In-context Instructions and How to Tackle It （ACL 2023）[<a href="https://aclanthology.org/2023.acl-long.703.pdf">paper link</a>]</h4> <p>这篇文章利用了Sup-NatInst的英文子集构建了更大的持续学习settting，使用500个训练任务和119个测评任务。对于每个训练任务并没有使用训练集中的所有实例，而是只是用100个或1-100随机数量个。在评估时关注学过的指令的<strong>实例级泛化</strong>和未学过的指令的<strong>任务级</strong>泛化。提出的解决方案包括约束模型在wide minima的正则化损失和动态重放。保持wide minima的正则化损失有点类似label smoothing。而动态重放则是研究保存什么样的样本用于重放，同时保存高熵和低熵的样本。</p> <p>我的个人经验是在需要评价泛化的benchmark中label smoothing很有用而会对下游任务表现有一定的牺牲。</p> <p><br/></p> <h4 id="mitigating-catastrophic-forgetting-in-large-language-models-with-self-synthesized-rehearsal-acl-2024paper-link">Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal （ACL 2024）[<a href="https://arxiv.org/abs/2403.01244">paper link</a>]</h4> <p>这篇文章尝试解决<strong>重放数据不可获取</strong>的情形。采用预训练的模型\(\theta^0\)与<strong>上下文学习</strong>生成指令学习的伪历史输入数据，并用\(\theta^{t-1}\)修正可能存在的错误。在重放时也进行选择，用聚类确定聚簇中心只重放在聚簇中心附近的数据。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250218210433279.png" alt="image-20250218210433279" style="zoom:40%;"/></p> <p><br/></p> <h4 id="orthogonal-subspace-learning-for-language-model-continual-learning-emnlp-2023-findings-paper-link">Orthogonal Subspace Learning for Language Model Continual Learning （EMNLP 2023 findings) [<a href="https://arxiv.org/abs/2310.14152">paper link</a>]</h4> <p>work by fdu</p> <p>提出O-LoRA使用LoRA来做持续指令微调。LoRA不同于其他PEFT的地方在于可以将LoRA的参数加回backbone避免参数增长。并没有采用重放而是使用正交正则化来减轻遗忘，想法是使得参数不同任务的更新方向相互正交，为此需要让LoRA的A矩阵列向量张成的子空间正交。</p> <p>类似于CVPR 2024的InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</p> <p>这种做法的问题在于当任务之间比较相似时，很难做到参数正交。</p> <p><br/></p> <h4 id="sapt-a-shared-attention-framework-for-parameter-efficient-continual-learning-of-large-language-models-acl-2024-paper-link">SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models （ACL 2024） [<a href="https://arxiv.org/abs/2401.08295">paper link</a>]</h4> <p>使用了SuperNI Benchmark和O-LoRA采用的15个分类任务的数据集，用PEFT结合作为学习方式。为每个task单独学习PEFT结构，并通过一个attention block以输入instance为Q，为每个PEFT结构分配的可学习的向量作为K构建查询，最终得到的attention系数对PEFT结构进行加权。为了减轻<strong>加权过程中的</strong>遗忘，重建伪样本并进行重放。做法类似于CVPR 2023的CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning。实验中发现<strong>LoRA的效果在指令微调数据集上比Prompt要好很多</strong>。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250219154425013.png" alt="image-20250219154425013" style="zoom:50%;"/></p> <p><br/></p> <h4 id="inscl-a-data-efficient-continual-learning-paradigm-for-fine-tuning-large-language-models-with-instructions-naacl-2024paper-link">InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions （NAACL 2024）[<a href="https://arxiv.org/abs/2403.11435">paper link</a>]</h4> <p>对持续指令微调中的重放机制做出改进。使用Wasserstein Distance衡量任务间相似度，加大与当前任务差异大的旧任务的重放数据比例，以此决定每个旧任务的重放规模。在此基础上使用<strong>指令</strong>指导重放政策。用 GPT-4 给指令打上若干细粒度标签，然后用类似于逆文档频率（IDF）的思路，将指令的复杂度和多样性量化为一个综合度量指标“InsInfo”。越复杂多样的指令，InsInfo值越高。在根据任务相似度分配好每个旧任务需要重放的规模之后，通过 InsInfo 将更多“高指令信息量”的样本选入重放集，从而得到更高效的重放。</p> <p>实验将将 SuperNI 中的 765 个英语任务整合为 16 个大类，backbone为lamma-7B。</p> <p><br/></p> <h4 id="还有两个没火的benchmark工作">还有两个没火的benchmark工作</h4> <ul> <li>TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models [<a href="https://arxiv.org/pdf/2310.06762.pdf">paper</a>] [<a href="https://github.com/BeyonderXX/TRACE">code</a>] （Trace不仅包含指令微调，指令微调只是其中的一条基线）</li> <li>CITB: A Benchmark for Continual Instruction Tuning [<a href="https://arxiv.org/pdf/2310.14510.pdf">paper</a>] [<a href="https://github.com/hyintell/CITB">code</a>]</li> </ul> <p><br/></p> <h3 id="43-continual-model-refinement-cmr">4.3 Continual Model Refinement (CMR)</h3> <p>模型在完成预训练被部署的时会被数据流\([x_0,x_1,…]\)不断测试并积累错误。对于其中的错误样本\(x_t, \hat{y}_t= f(x_t)\ne y_t\)，我们将其加入到模型精炼的数据集\(\mathcal{X}_{e(rror)}\)中。模型精炼（Model Refinement）就是在测试时错误的数据子集上微调模型并修正错误的过程。</p> <p>对持续模型精炼（Continual Model Refinement ）而言，这个精炼的过程会不断进行，模型需要在不遗忘正确答案的前提下不断修正错误。相较于直接微调模型，更多的方法采用<strong>模型编辑</strong>技术。对于模型编辑技术而言，可以改变指定样本的输出而不影响到其他样本的输出。这样的setting也被称为Continual Model Editing。</p> <p>对模型编辑技术而言，实验结果在自己的setting上都做得很好。但是似乎没有文章讨论进行编辑的最优位置。</p> <p><br/></p> <h4 id="on-continual-model-refinement-in-out-of-distribution-data-streams-acl-2022paper-link">On Continual Model Refinement in Out-of-Distribution Data Streams （ACL 2022）[<a href="https://arxiv.org/abs/2205.02014">paper link</a>]</h4> <p>work by Facebook AI</p> <p>这篇文章提出了Continual Model Refinement的新setting。NLP服务在离线训练并被部署后，会在OOD的数据流（查询数据流）上被不断使用和测试，并被记录犯错的数据（错误数据流），此时需要<strong>模型精炼</strong>只在错误的数据上训练，在不引发遗忘的前提下修正错误。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250219165116874.png" alt="image-20250219165116874" style="zoom:50%;"/></p> <p>CMR的设定为在线学习。首先有上游预训练。完成预训练后再下游的<strong>查询数据流</strong>上不断被测试，其中的错误被标注并收集得到<strong>错误数据流</strong>。模型需要不断修复错误数据流中的错误。作者使用extractive question answering任务MRQA-19的6个数据集构建一个离线上游预训练数据集并用另外5个数据集构建分布外数据流。在此基础上提出了一种通用的采样算法来在多个 OOD 数据集（多个子分布）之间切换或混合生成数据流，并在早期阶段包含一定比例的“上游分布”样本（逐步衰减），以模拟真实世界中数据混杂且分布动态演变的情况。论文设计了五个核心指标以衡量 CMR 方法的性能：</p> <ul> <li><strong>Error-Fixing Rate (EFR)</strong>：模型在每个时间步上对当前新错误样本的即时修复率；</li> <li><strong>Upstream Knowledge Retention (UKR)</strong>：模型对初始上游训练数据的记忆保留程度；</li> <li><strong>Online Knowledge Retention (OKR)</strong>：模型对先前流数据里已学会的样本的记忆程度；</li> <li><strong>Cumulative Success Rate (CSR)</strong>：对持续到来的流数据实时预测的累计正确率；</li> <li><strong>Knowledge Generalization (KG)</strong>：模型在额外抽取的、分布相似但未被用于训练的 OOD 测试集上的表现，用于衡量模型对未见样本的泛化能力。</li> </ul> <p>方法部分只基于传统CL方法做了几个简单的baseline。</p> <p><br/></p> <h4 id="aging-with-grace-lifelong-model-editing-with-discrete-key-value-adaptors-neurips-2023paper-link">Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors （NeurIPS 2023）[<a href="https://arxiv.org/abs/2211.11031">paper link</a>]</h4> <p>work by MIT</p> <p>solid work。使用model editing技术做CMR。提出General Retrieval Adaptors for Continual Editing（GRACE），在模型的中间层加入若干可学习的Adopter结构。Adopter结构缓存错误样本的隐藏层标识并学习新的表示，仅在输入与缓存的隐藏层表示相似时才被激活（通过作用半径阈值控制）。最终实现的效果是只改变需要修正的样本的输出，而其他正确样本的输出不改变。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250219195241105.png" alt="image-20250219195241105" style="zoom:50%;"/></p> <p>最终在三个setting上进行了实验T5 进行无上下文 QA（zsRE 数据集）、BERT 进行法律文本多分类（SCOTUS 数据集）、GPT-2 进行文本生成的幻觉修正（Hallucination 数据集）。这三个都是model editing的setting。</p> <p><br/></p> <h4 id="larimar-large-language-models-with-episodic-memory-control-icml-2024paper-link">Larimar: Large Language Models with Episodic Memory Control （ICML 2024）[<a href="https://arxiv.org/abs/2403.11901">paper link</a>]</h4> <p>受到持续学习complementary learning systems理论的启发，使用外部存储器来保存短时情景记忆。在模型推理时，会从存储器中获取对应的记忆来修正输出。由于是一种<strong>train-free</strong>的方式，取得了更高的效率。存储单元的更新方式类似于Kanerva Machine（这是什么？）。</p> <p><br/></p> <h4 id="wise-rethinking-the-knowledge-memory-for-lifelong-model-editing-of-large-language-models-neurips-2024paper-link">WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models （NeurIPS 2024）[<a href="https://arxiv.org/abs/2405.14768">paper link</a>]</h4> <p>work by ZJU</p> <p>作者认为持续模型编辑中<strong>可靠性</strong>、<strong>泛化性</strong> 和 <strong>局部性</strong>存在不可能三角。这是因为长期记忆（预训练模型参数）和工作记忆（通过检索激活的知识）之间很难权衡。对GRACE而言，基于隐藏层表示相似度的激活方式似乎很难泛化到没见过的样本上。这篇工作提出了WISE，复制LLM的部分FFN作为可编辑的部分，并引入用对比损失训练的路由机制，使得修改后的FFN只在遇到训练集相似样本时被激活。这个想法和GRACE差不多只是训练方式不同。但是这样做已经能保证可靠性和局部性。作者认为保持泛化性的关键在于工作记忆的知识密度，并提出了一种分片和整合的机制保证泛化性。就实验结果而言似乎性能已经很优秀了。但是模型编辑在OOD的任务上的泛化能力依旧比较差（文章table 5）。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250220152516929.png" alt="image-20250220152516929" style="zoom:40%;"/></p> <p><br/></p> <h3 id="44-continual-model-alignment-cma">4.4 Continual Model Alignment (CMA)</h3> <p>在通过人类反馈强化学习（HFRL）对齐模型或通过微调做伦理对齐时会导致模型遗忘预训练知识，被称为alignment tax，即垂直遗忘。同时RLHF-based LLM本身也需要不断从新的查询和反馈中学习因而存在水平遗忘。Continual Model Alignment的研究同时包含对这两种遗忘的研究。目前的工作主要聚焦于HFRL带来的垂直遗忘和水平遗忘。</p> <h4 id="mitigating-the-alignment-tax-of-rlhfemnlp-2024">Mitigating the Alignment Tax of RLHF（EMNLP 2024）</h4> <h4 id="cppo-continual-learning-for-reinforcement-learning-with-human-feedback-iclr-2024">CPPO: Continual Learning for Reinforcement Learning with Human Feedback (ICLR 2024)</h4> <p><br/></p> <h2 id="5-数据集和评价指标">5 数据集和评价指标</h2> <p>Continual Learning of Large Language Models一文中统计了各种工作使用的benchmark，发现大家基本都是各玩各的。</p> <p><img src="/assets/img/post_img/2025-02-24-continual-learning-of-llms-survey/image-20250224012733471.png" alt="image-20250224012733471" style="zoom:50%;"/></p> <p>对于评价指标，一般是在单任务指标的基础上在每个时间步对所有任务进行测试得到下三角矩阵，按照传统持续学习计算Last、Avg.和Forget指标的方式类比计算。由于LLM具有较强泛化能力，一些工作还引入了OOD泛化能力的测评。</p>]]></content><author><name></name></author><category term="paper-sharing"/><category term="continual-learning"/><category term="LLM"/><summary type="html"><![CDATA[1 引言]]></summary></entry><entry><title type="html">LLM for education 调研</title><link href="https://luo-jiaming.github.io/blog/2025/llm-for-education-survey/" rel="alternate" type="text/html" title="LLM for education 调研"/><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>https://luo-jiaming.github.io/blog/2025/llm-for-education-survey</id><content type="html" xml:base="https://luo-jiaming.github.io/blog/2025/llm-for-education-survey/"><![CDATA[<p>近年来，随着大规模语言模型（LLM）的迅速发展，人工智能在教育领域的应用正迎来前所未有的机遇。LLM在辅助学生学习、辅助教师教学方面都起到巨大的作用，并允许为学生定制个性化的学习路线。本文基于Squirrel AI的<a href="https://arxiv.org/abs/2403.18105">综述文章</a>和收集相关论文的<a href="https://github.com/Geralt-Targaryen/Awesome-Education-LLM">github仓库</a>对LLM for education进行调研。</p> <p>根据综述文章，LLM在教育界的应用主要可以分为四类：首先是<strong>学生辅助</strong> (Student Assistance)，包括问题求解 (Question Solving)、错误纠正 (Error Correction) 和困惑辅助 (Confusion Helper)，旨在为学生提供答疑与个性化反馈；其次是<strong>教师辅助</strong> (Teacher Assistance)，涵盖问题生成 (Question Generation)、自动评分 (Automatic Grading) 及教材生成 (Material Creation)，帮助教师减轻日常教学负担；第三是<strong>自适应学习</strong> (Adaptive Learning)，利用知识追踪 (Knowledge Tracing) 和内容个性化 (Content Personalizing) 根据学生表现定制学习路径或适应学生水平的材料；最后是具体落地的<strong>教育工具包</strong> (Education Toolkit)，为构建智能化教学生态系统提供全面支持。此外，<strong>AIGC Detection in Education</strong>也是一个比较有意思且具有应用价值的领域。下图是综述文章中的分类图。</p> <p>PS：ACL似乎有过Building Educational Applications的workshop。以及International conference on artificial intelligence in education, AIED 以及 International Conference on Educational Data Mining, EDM 好像认可度也比较高。</p> <p><img src="/assets/img/post_img/2025-02-24-llm-for-education-survey/image-20250220160437870.png" alt="image-20250220160437870" style="zoom:40%;"/></p> <center>综述中的LLM4EDU分类</center> <p><br/></p> <h2 id="1-学生辅助">1 学生辅助</h2> <p>LLM通过生成类似于人类的问答能帮助学生解决问题，指出错误或解答疑难问题。</p> <p><br/></p> <h3 id="11-question-solving-qs">1.1 Question Solving (QS)</h3> <p>实际上感觉不只是教育领域的问题，而是在于提升LLM在某一领域的<strong>专业知识</strong>以及解决复杂问题的<strong>推理能力</strong>。一方面可以通过领域适应微调或预训练来丰富专业知识，另一方面可以通过思维链CoT增强推理能力以及强化LLM对指令的理解能力。这些我觉得找一个<strong>强大的预训练模型</strong>就能做到。为了避免LLM产生运算错误，需要借助<strong>外部工具</strong>进行计算（<a href="http://arxiv.org/pdf/2211.10435">PAL: Program-aided Language Models</a>；<a href="https://arxiv.org/abs/2308.07921">Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</a>）。</p> <p><br/></p> <h3 id="12-error-correction-ec">1.2 Error Correction (EC)</h3> <p>针对学生提交的文本进行修复，例如修复学生文章或代码的语法错误，依靠微调预训练的语言模型来实现。<a href="https://arxiv.org/abs/2307.13923">GrammarGPT</a>利用 LLM 来解决中文语法错误，通过使用混合注释数据集（包括人工注释和 ChatGPT 生成）对开源 LLM 进行微调，所提出的框架在母语中文语法错误纠正方面表现良好。<a href="https://arxiv.org/abs/2209.14876">MMAPR</a>使用在代码中训练的大型语言模型（如 Codex）为入门级 Python 编程作业构建 <strong>自动程序修复</strong>APR 系统（MMAPR），通过在真实学生程序上对 MMAPR 进行评估并将其与之前最先进的 Python 语法修复引擎进行比较，作者发现 MMAPR 可以修复更多程序，并且平均生成更小的补丁。这更贴近于<strong>软件工程</strong>方向的工作。</p> <p>关于语法修正有<strong>落地应用grammarly</strong>。</p> <p><br/></p> <h3 id="13-confusion-helper-ch">1.3 Confusion Helper (CH)</h3> <p>与前两个方向不同，LLM不应该直接生成答案而应该生成<strong>建议和提示</strong>（hint）帮助引导学生自己解决问题。</p> <p><strong>Automatic Generation of Socratic Subquestions for Teaching Math Word Problems, EMNLP2022 (<a href="https://arxiv.org/abs/2211.12835">link</a>)</strong> 提出使用苏格拉底引导式提问的方式，对于输入的数学问题输出若干子问题。整套方法包含三个主要模块：<strong>内容规划（Content Planner）</strong>、问题生成（Question Generator, QG）以及<strong>奖励策略（Rewards for Reinforcement Learning）</strong>。在<strong>内容规划阶段</strong>使用轻量的seq2seq模型提取题目中的运算与方程式，然后与原始问题一起输入骨干网络（T5）<strong>生成子问题</strong>，在此基础上进一步使用<strong>强化学习策略</strong>进一步提升效果。实验发现在简单问题上表现好而对于复杂的问题，生成的子问题甚至可能具有误导性。</p> <p>类似的现象在<strong>Learning gain differences between ChatGPT and human tutor generated algebra hints</strong> (<a href="https://arxiv.org/abs/2302.06871">link</a>) 中也被观察到。由LLM生成的对问题的hint与人类教师提供的亦有较大差距。LLM生成的提示通常比较general，且包含学生无法理解的专有名词。</p> <p>LLM表现较差的原因可能是因为LLM对用户（学生）的知识水平的了解程度较低。为此<strong>Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?</strong> (<a href="http://arxiv.org/abs/2312.02065">link</a>) 在问题中加上表明读者受教育水平的prompt并对输出回答进行可读性分析，评估ChatGPT 与、GPT-3、Flan-T5、BigScience T0模型，发现四个模型（仅通过提示工程）大都呈现出各自固定的可读性输出区间，对于特定年龄、教育程度的提示并没有很好地自适应。当然目前的测评都是基于比较老的模型了，说不定最新的模型能做的更好。</p> <p><br/></p> <h2 id="2-教师辅助">2 教师辅助</h2> <p>教师的日常工作往往包括出题、备课、作业批改、素材制作等重复性任务。LLM 的出现，为教师减负带来新的可能性。在综述文章中对于问题生成Question Generation和教学材料生成Material Creation的内容有所重叠，而且教学材料生成部分综述调研的文章质量都比较低，我们将其直接抛弃，并且列出一些在问题生成和自动评分以外的其他工作。</p> <p><br/></p> <h3 id="21-question-generation-qg">2.1 Question Generation (QG)</h3> <p>由于在教学实践中经常使用，问题生成 (QG) 已成为 LLM 在教育应用中最受欢迎的研究课题之一。许多工作聚焦于某些特定学科领域，如Language或Computer Science或Math等。</p> <p><strong>Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications</strong>, ACL BEA Workshop, 2023, THU (<a href="https://aclanthology.org/2023.bea-1.52/">link</a>) 利用 LLM （在课本和补充材料微调gpt2）生成阅读理解材料和问题，并实现了一个允许教师操作和再编辑的系统（很简单的demo）。这个工作还是比较有价值的。</p> <p><strong>Student Answer Forecasting: Transformer-Driven Answer Choice Prediction for Language Learning</strong>, EDM 2024, <a href="https://arxiv.org/abs/2405.20079">link</a> 通过real-world German language learning ITS中超过10,000学生的真实选择表现构建学生行为的表示，能够把握学生容易犯错的概念，从而生成更有考察点的多选题MCQ。</p> <p><strong>A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education</strong> (<a href="https://arxiv.org/abs/2312.03173">link</a>) 收集了四门 Python 编程和两门数据科学相关课程的 246 个模块级学习目标（LOs）将每个 LO 进行自动分类（如“记忆”、“理解”、“应用”等），并基于分类结果来确定具体的题目类型（如简单回忆题、填空题、代码输出题、情景题等）构建提示工程直接让GPT4进行生成，发现GPT-4 生成的选择题与人工题目相差不大，其中 81.7% 的自动生成题完全满足所有质量标准。</p> <p><strong>Learning by Analogy: Diverse Questions Generation in Math Word Problem, ACL 2023 findings</strong> (<a href="https://arxiv.org/abs/2306.09064">link</a>) 作者的本意似乎是现在的数学问题数据集只包含“单一问题”与对应“等式/答案”而缺乏可以激发模型类比的多样化能力。所以提出一种包含多种生成模式的Diverse Equations Generator来生成多样的题目。好像对education的问题生成也有点帮助。</p> <p>LLM也具有生成练习题以外的其他教学材料的潜力。</p> <p><strong>Generating Educational Materials with Different Levels of Readability using LLMs</strong>, <a href="https://arxiv.org/abs/2406.12787">link</a></p> <p>研究旨在解决教育材料的可读性调整问题，特别是如何根据不同学生的阅读能力生成适当难度的文本。提出了一个“分级文本生成”任务，即根据目标可读性水平（如Lexile评分）重写原始教育文本，同时保持原意。研究使用了三种LLM（GPT-3.5、LLaMA-2 70B和Mixtral 8x7B）进行生成，采用零-shot和少量示例学习（few-shot learning）方法。通过Lexile进行可读性评分。</p> <p><br/></p> <h2 id="22--automatic-grading-ag">2.2 Automatic Grading (AG)</h2> <p>在 LLM 出现之前，关于自动作业评分系统的研究就已经提出了。然而，由于先前模型学习能力的限制，大多数现有的自动评分算法如<strong>Automatic short answer grading via multiway attention networks</strong> (<a href="https://arxiv.org/abs/1909.10166">link</a>) 侧重于探索<strong>黄金解答</strong>和<strong>学生答案</strong>之间的语义比较（现在的很多生成式模型就是这么被评估的），而忽略了手动评分过程背后的逻辑考虑。除此之外，提供的解决方案的质量对结果有很大影响（标注要求高）。随着 LLM 的出现，上述挑战变得容易解决。</p> <p><strong>Large language models for education: Grading open-ended questions using chatgpt</strong>, 2023, <a href="https://arxiv.org/abs/2307.16696">link</a></p> <p><strong>Rating short L2 (second-language) essays on the cefr scale with gpt-4,</strong> ACL BEA Workshop, 2023, <a href="https://aclanthology.org/2023.bea-1.49.pdf">link</a></p> <p>以上两个研究首先探索了使用<strong>提示微调</strong>让LLM分别评分<strong>开放式问题</strong>和<strong>文章写作</strong>。通过提供全面的上下文、清晰的评分标准和高质量的示例，LLM 在两个评分任务上都表现出了令人满意的性能。</p> <p><strong>From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape</strong>, thu, 2024, <a href="https://arxiv.org/html/2401.06431v1">link</a> 将 CoT 整合到评分过程中。这种方法要求 LLM 首先分析和解释所提供的材料，然后再确定最终分数。经过这样的修改，LLM 不仅会生成分数结果，还会对学生的答案提供详细的评论，帮助学生学习如何在下一次改进。</p> <p><strong>LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought</strong>, IJCAI 2024, <a href="https://arxiv.org/abs/2405.06705">link</a> 这篇文章并不是LLM for ED而是ED for LLM。运用教育学的<strong>布卢姆认知模型</strong>（Bloom Cognitive Model，BCM), 提出一种Pedagogical Chain-of-Thought (PedCoT)的方法用于模型在数学推理中的错误纠正。通过prompt让大模型按照数学概念、解题思路、计算过程的步骤去分阶段思考。</p> <p><strong>Automated Feedback for Student Math Responses Based on Multi-Modality and Fine-Tuning</strong> 将评分对象从学生的文本答案扩展到手写答案。通过使用先进的多模态框架（例如 CLIP 和 BLIP），发现学生的文本和图像以及问题的文本和图像的结合可以提高模型的评分性能。(CLIP和BLIP不算LLM吧)</p> <p><strong>Pronunciation Assessment with Multi-modal Large Language Models, zuoyebang, 2024</strong>, <a href="https://arxiv.org/abs/2407.09209">link</a> 提出利用“LLM + 语音编码器 + 模态适配器”的多模态方法，尝试让LLM直接根据音频和对应的文本预测发音准确度与流利度分数。</p> <p><strong>Reducing the cost: Cross-prompt pre-finetuning for short answer scoring</strong>, AIED, 2024, <a href="https://arxiv.org/abs/2408.13966">link</a> 通过<strong>跨题目</strong>的数据训练一个模型，并且在新题目上实现良好的评分效果。<strong>预微调阶段</strong>在已有的多个题目（跨题目数据）上进行模型的预微调。此阶段使用了不同题目的标注数据，这些题目的评分标准和答案内容不同，但可以通过学习通用的评分原则（例如，答案中包含评分标准中指定的信息就会得分更高）来提高模型的泛化能力。然后，使用新的题目数据对预微调后的模型进行微调。此阶段仅使用新的题目数据（不需要访问跨题目数据），目的是让模型适应新的评分标准。类似transfer learning。</p> <p><br/> <img src="/assets/img/post_img/2025-02-24-llm-for-education-survey/image-20250223192541518.png" alt="image-20250223192541518" style="zoom:50%;"/></p> <center>综述中的LLM在学生和教师辅助中的应用</center> <p><br/></p> <h3 id="23-others">2.3 Others</h3> <p><strong>Social Skill Training with Large Language Models</strong>, work by cmu, <a href="https://arxiv.org/abs/2404.04204">link</a></p> <p>论文提出了一个名为“AI Partner和AI Mentor”（APAM）框架的社交技能培训方案。这个框架结合了两种人工智能角色：<strong>AI Partner</strong>：通过模拟对话，提供情景化的练习和体验，帮助学习者在低风险环境下练习社交技能。<strong>AI Mentor</strong>：提供基于领域知识和实际经验的个性化反馈，帮助学习者改进其社交技能。</p> <p><strong>Simulating Classroom Education with LLM-Empowered Agents</strong>, work by thu, <a href="https://arxiv.org/abs/2406.19226">link</a></p> <p>论文提出了一个名为SimClass的多代理虚拟课堂框架。该框架通过引入不同的课堂角色（如教师、助教和同学）模拟课堂上的师生互动和学生之间的互动。每个代理都被赋予了特定的角色和行为，以确保课堂互动的丰富性和多样性。SimClass的设计不仅关注教学内容的传递，还强调通过互动增强学生的学习体验。（那么问题来了，为什么不和真实的老师和同学互动？）</p> <p><img src="/assets/img/post_img/2025-02-24-llm-for-education-survey/image-20250223191211808.png" alt="image-20250223191211808" style="zoom:50%;"/></p> <p><br/></p> <h2 id="3-自适应学习">3 自适应学习</h2> <p><strong>知识追踪</strong>目标是根据学生在学习过程中对问题的回答的正确性来估算学生知识掌握状态的目标。<strong>内容个性化</strong>专注于根据个性化因素（例如学习状况，偏好和目标）为学生提供定制的学习内容。可以将LLM与<strong>知识图谱</strong>结合。</p> <p><br/></p> <h3 id="31-knowledge-tracing-kt">3.1 Knowledge Tracing (KT)</h3> <p>LLM在知识跟踪中的当前使用侧重于根据问题文本和学生使用记录数据生成学生对知识掌握情况的预测（预测学生对特定题目的作答表现或是预测题目的难度等）。本质上还是分类任务。主要用于<strong>推荐</strong>。</p> <p><strong>Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy,</strong> AAAI 2024, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/30370">link</a> 在线教育通过积累大量题目和学生的回答获得了大量的数据。能通过这些历史记录预测学生的做答表现对于个性化学习、智能推荐与后续教学策略具有重要意义。这篇文章提出使用符号图神经网络SGNN（将学生与试题分别视作二部图两侧的节点；学生对题目的回答被转换为“正确（正边）”或“错误（负边）”，从而构造出一个<strong>带符号的二部图</strong>。预测学生作答正确或错误即为带符号链接预测问题）+ LLM提取的题目特征表示完成这一任务。</p> <p><strong>Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction</strong>, <a href="https://arxiv.org/abs/2312.11890">link</a> 这篇文章尝试预测题目的难度。分别预测题目与概念的难度（即到底是知识点难还是题目难）。用对比学习框架CL4KT对题目的难度进行正负映射。利用MonaCoBERT将题目 ID、概念 ID、难度值（包含正、负两个版本）、学生回答的正误信息等分别映射到向量空间。这篇工作应该是下面两篇工作的杂糅：<strong>Monacobert: Monotonic attention based convbert for knowledge tracing</strong> 和 <strong>Contrastive learning for knowledge tracing</strong>。</p> <p><strong>Estimating Difficulty Levels of Programming Problems with Pre-trained Models</strong>, <a href="https://arxiv.org/abs/2406.08828">link</a>, 论文提出了一种自动化评估编程题目难度的方法，通过充分利用题目描述的文本信息以及示例代码的语义，借助预训练LLM来快速客观地预测题目难度。构建了题目文本（BERT）、示例代码（CodeBERT）、和资源限制等多个模态特征的输入。</p> <p><br/></p> <h3 id="32-content-personalizing-cp">3.2 Content Personalizing (CP)</h3> <p>最近的教育研究中探索了LLM来创建个性化学习内容（<strong>个性化生成</strong>）。</p> <p><strong>Leveraging LLMs for Adaptive Testing and Learning in Taiwan Adaptive Learning Platform (TALP)</strong> 根据学生的最新知识掌握诊断结果为学生生成动态学习路径。</p> <p><strong>An LLM-Powered Adaptive Practicing System</strong> 根据学习目标Lo自动为学生生成下一个学习目标的问题。</p> <p><strong>Contextualizing problems to student interests at scale in intelligent tutoring system using large language models</strong> 探讨了LLM在基于学生兴趣的情况下创建情境化代数问题的潜力。通过设计和不断优化提示语（prompt），使 GPT-4 能在不改变原有题目数值和难度的前提下，将题目背景转换为与学生兴趣相关的情境。例如，将原本的代数题目分别转换成与视频游戏、TikTok和NBA等兴趣相关的题目。这有助于改善研究期间的学生参与度和结果。</p> <p><strong>Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations</strong> 利用基于聊天的LLM来生成学习建议。从预先定义的教育资源和课程体系中构建KG，提取学习目标、课程、主题和开放教育资源之间的层级结构、语义关系和元数据。将KG中提取的结构化和语义信息转化为文本上下文，嵌入到GPT-4的提示语中。（知识图谱+LLM）</p> <p><br/></p> <h2 id="4-教育工具包">4 教育工具包</h2> <p>现在的很多LLM应用如GPT系列、DeepSeek、Bing Chat等聊天机器人基于RolePlay的提示工程本身就能很好地扮演教师角色。一些其他LLM赋能的工具如Grammarly本身对教育也有所帮助。</p> <p>综述中列举了一些盈利性质的网站型应用如</p> <ul> <li> <p><a href="https://beta.diffit.me/">Diffit</a>: 根据输入的问题教育水平可以生成教材、插图和练习题。并且支持提取其中的一些生词。支持后期编辑。</p> </li> <li> <p><a href="https://www.magicschool.ai/">MagicSchool</a>: 集成教师端和学生端的综合平台，包括多种AI工具。声称可以帮助教师制定课程计划、设计作业、生成材料、创建简报和其他几项任务的工具，每周可为教育工作者节省多达 10 个小时的时间。</p> </li> <li> <p><a href="https://educationcopilot.com/">Education Copilot</a>: 使用copilot提供教育相关文档写作的帮助。</p> </li> </ul> <p>等等（感觉都像是会出现在youtube广告中的东西）。我觉得这些的参考价值不大。</p> <p><br/></p> <h2 id="5-挑战与发展方向">5 挑战与发展方向</h2> <p>直接概括了综述中总结的挑战与发展方向。</p> <p>当前的挑战：</p> <ol> <li><strong>偏见和公平性问题</strong>：LLM可能因训练数据中的不平衡而产生偏见，影响模型生成的内容的公平性和包容性，尤其是针对不同群体（如少数族裔或非英语使用者）。</li> <li><strong>可靠性和安全性问题</strong>：LLM的“幻觉”问题（生成不真实的内容）及其生成有毒、无一致性的输出，对教育应用中的使用造成风险。</li> <li><strong>透明度和问责制</strong>：LLM通常是黑箱操作，这导致在教育应用中，用户难以理解模型的决策过程，也增加了抄袭、考试作弊等问题的风险。</li> <li><strong>隐私和安全问题</strong>：随着LLM的广泛应用，尤其在教育领域，如何保护学生的个人隐私成为一个重要议题。</li> <li><strong>过度依赖LLM</strong>：学生可能过度依赖LLM，削弱了独立思考和学术写作等关键能力，尤其是当学生不进行有效的批判性思维训练时。</li> </ol> <p>未来的发展方向：</p> <ol> <li><strong>教育目标对齐的LLM</strong>：未来可以利用增强生成技术（如检索增强生成技术，RAG）让LLM更好地与教育目标对齐，产生符合教育需求的输出。</li> <li><strong>多智能体教育系统</strong>：使用多个LLM代理共同协作解决教育中的复杂任务，尤其是在自动评分和批改等任务中，通过多代理合作提高任务处理的精确度。</li> <li><strong>多模态和多语言支持</strong>：LLM不仅能够处理文本信息，还能结合图像、音频等多模态输入，提供更加丰富和个性化的学习体验。同时，支持多语言的LLM将促进全球教育资源的普及。</li> <li><strong>边缘计算和效率提升</strong>：通过在边缘计算环境中部署LLM，可以提高教育技术的效率，并且减少对网络带宽的依赖，保障数据安全。（意义不大，我觉得边缘设备通过联网调api比直接部署更现实）</li> <li><strong>专用领域模型的高效训练</strong>：开发适应特定教育领域的专用LLM，通过精准的学科训练，提高模型的专业性和成本效益。</li> </ol>]]></content><author><name></name></author><category term="paper-sharing"/><category term="LLM-for-Education"/><summary type="html"><![CDATA[近年来，随着大规模语言模型（LLM）的迅速发展，人工智能在教育领域的应用正迎来前所未有的机遇。LLM在辅助学生学习、辅助教师教学方面都起到巨大的作用，并允许为学生定制个性化的学习路线。本文基于Squirrel AI的综述文章和收集相关论文的github仓库对LLM for education进行调研。]]></summary></entry></feed>